{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gw-YNUEkVCNQ","executionInfo":{"status":"ok","timestamp":1707358717406,"user_tz":-120,"elapsed":3923,"user":{"displayName":"Team Project","userId":"02515086979699117879"}},"outputId":"2340a53a-4fa0-4d01-ca3b-be68540ffb01"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]}],"source":["import tensorflow as tf\n","gpus = tf.config.list_physical_devices('GPU')\n","\n","if gpus:\n","    print(\"GPU available:\", gpus)\n","else:\n","    print(\"No GPU available.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40294,"status":"ok","timestamp":1709912212755,"user":{"displayName":"Team Project","userId":"02515086979699117879"},"user_tz":-120},"id":"0y-fOU2VVDiq","outputId":"2c96191b-15fe-4de9-94d8-e1270c6485d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["# Step 2: Data Preparation\n","!unzip -q '/content/drive/MyDrive/IMG_CLASSES.zip' -d '/content/dataset'"],"metadata":{"id":"DCdNrsGZgYK9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import shutil\n","import random\n","\n","class_names = [\n","    \"Atopic Dermatitis\", \"Basal Cell Carcinoma (BCC)\", \"Benign Keratosis-like Lesions (BKL)\", \"Eczema\",\n","    \"Melanocytic Nevi (NV)\", \"Melanoma\", \"Psoriasis pictures Lichen Planus and related diseases\",\n","    \"Seborrheic Keratoses and other Benign Tumors\", \"Tinea Ringworm Candidiasis and other Fungal Infections\",\n","    \"Warts Molluscum and other Viral Infections\"\n","]\n","\n","# Define the target number of images per class\n","target_num_images = 1200\n","# Define the base directory containing your dataset\n","base_dir = '/content/dataset/IMG_CLASSES'  # Update to your dataset directory\n","\n","# Create a new directory to store the balanced dataset\n","balanced_dir = '/content/dataset/IMG_CLASSES_BALANCED'  # Update to your desired directory\n","if not os.path.exists(balanced_dir):\n","    os.makedirs(balanced_dir)\n","\n","# Loop through each class folder in the original dataset\n","for class_name in class_names:\n","    class_dir = os.path.join(base_dir, class_name)\n","    images = os.listdir(class_dir)\n","\n","    # Shuffle the images randomly\n","    random.shuffle(images)\n","\n","    # Determine the number of images to keep for this class\n","    num_images_to_keep = min(len(images), target_num_images)\n","\n","    # Create a new directory for this class in the balanced dataset\n","    balanced_class_dir = os.path.join(balanced_dir, class_name)\n","    if not os.path.exists(balanced_class_dir):\n","        os.makedirs(balanced_class_dir)\n","\n","    # Copy a subset of images to the balanced dataset directory\n","    for image in images[:num_images_to_keep]:\n","        src = os.path.join(class_dir, image)\n","        dst = os.path.join(balanced_class_dir, image)\n","        shutil.copyfile(src, dst)\n"],"metadata":{"id":"xCQ0l76FgVlQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Define the directory containing your balanced dataset\n","balanced_dir = '/content/dataset/IMG_CLASSES_BALANCED'  # Update to your balanced dataset directory\n","\n","# Loop through each class folder in the balanced dataset\n","for class_name in os.listdir(balanced_dir):\n","    # Make sure we're only counting files in directories (class folders)\n","    if os.path.isdir(os.path.join(balanced_dir, class_name)):\n","        class_dir = os.path.join(balanced_dir, class_name)\n","        images = os.listdir(class_dir)\n","        num_images = len(images)\n","        print(f\"{class_name}: {num_images} images\")\n","\n","        # Optionally, assert to make sure each class has exactly 1200 images\n","        assert num_images == 1200, f\"{class_name} does not have 1200 images, it has {num_images} images.\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eaSju2wLhEDg","executionInfo":{"status":"ok","timestamp":1707358964529,"user_tz":-120,"elapsed":539,"user":{"displayName":"Team Project","userId":"02515086979699117879"}},"outputId":"f35a5c26-e969-4b52-f081-e6da4398dcb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Atopic Dermatitis: 1200 images\n","Melanocytic Nevi (NV): 1200 images\n","Benign Keratosis-like Lesions (BKL): 1200 images\n","Psoriasis pictures Lichen Planus and related diseases: 1200 images\n","Warts Molluscum and other Viral Infections: 1200 images\n","Eczema: 1200 images\n","Tinea Ringworm Candidiasis and other Fungal Infections: 1200 images\n","Seborrheic Keratoses and other Benign Tumors: 1200 images\n","Basal Cell Carcinoma (BCC): 1200 images\n","Melanoma: 1200 images\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XWZsrAZtVPLV"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","import os\n","import shutil\n","\n","# Define class names and the number of images per class\n","class_names = [\n","    \"Atopic Dermatitis\", \"Basal Cell Carcinoma (BCC)\", \"Benign Keratosis-like Lesions (BKL)\", \"Eczema\",\n","    \"Melanocytic Nevi (NV)\", \"Melanoma\", \"Psoriasis pictures Lichen Planus and related diseases\",\n","    \"Seborrheic Keratoses and other Benign Tumors\", \"Tinea Ringworm Candidiasis and other Fungal Infections\",\n","    \"Warts Molluscum and other Viral Infections\"\n","]\n","\n","class_image_counts = [1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200]\n","\n","# Organize the data into training and testing sets\n","base_dir = '/content/dataset/IMG_CLASSES_BALANCED'  # Update to your dataset directory\n","\n","# Create train and test directories\n","train_dir = os.path.join(base_dir, 'train')\n","test_dir = os.path.join(base_dir, 'test')\n","\n","if not os.path.exists(train_dir):\n","    os.makedirs(train_dir)\n","\n","if not os.path.exists(test_dir):\n","    os.makedirs(test_dir)\n","\n","# Move images into train and test directories\n","for class_name, image_count in zip(class_names, class_image_counts):\n","    class_dir = os.path.join(base_dir, class_name)\n","    images = os.listdir(class_dir)\n","\n","    # Split the images into training and testing sets\n","    train_images, test_images = train_test_split(images, test_size=0.3, random_state=42)\n","\n","    # Create class-specific directories in the train and test directories\n","    train_class_dir = os.path.join(train_dir, class_name)\n","    test_class_dir = os.path.join(test_dir, class_name)\n","\n","    if not os.path.exists(train_class_dir):\n","        os.makedirs(train_class_dir)\n","    if not os.path.exists(test_class_dir):\n","        os.makedirs(test_class_dir)\n","\n","    # Move images to their respective directories\n","    for image in train_images:\n","        src = os.path.join(class_dir, image)\n","        dst = os.path.join(train_class_dir, image)\n","        shutil.move(src, dst)\n","\n","    for image in test_images:\n","        src = os.path.join(class_dir, image)\n","        dst = os.path.join(test_class_dir, image)\n","        shutil.move(src, dst)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"s_B6v4-tgO0x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","import os\n","\n","# Function to resize images in a directory\n","def resize_images(directory, target_size):\n","    for root, dirs, files in os.walk(directory):\n","        for file in files:\n","            filepath = os.path.join(root, file)\n","            try:\n","                img = Image.open(filepath)\n","                img = img.resize(target_size, Image.ANTIALIAS)\n","                img.save(filepath)\n","            except Exception as e:\n","                print(f\"Failed to process {filepath}: {e}\")\n","\n","# Define target size for resizing\n","target_size = (512, 512)  # Set your desired dimensions here\n","\n","# Paths to train and test directories\n","train_directory = '/content/dataset/IMG_CLASSES_BALANCED/train'\n","test_directory = '/content/dataset/IMG_CLASSES_BALANCED/test'\n","\n","# Resize images in train directory\n","resize_images(train_directory, target_size)\n","\n","# Resize images in test directory\n","resize_images(test_directory, target_size)\n"],"metadata":{"id":"6Plt6TJbd0ox","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707359381650,"user_tz":-120,"elapsed":193705,"user":{"displayName":"Team Project","userId":"02515086979699117879"}},"outputId":"8f1becc9-8a9c-473c-fd9b-956cd5a3698b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-10-02b14f87be96>:11: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n","  img = img.resize(target_size, Image.ANTIALIAS)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12822,"status":"ok","timestamp":1707359408708,"user":{"displayName":"Team Project","userId":"02515086979699117879"},"user_tz":-120},"id":"daXhhL6UV2f-","outputId":"f7afde11-d5ce-4270-b860-f4184fce6083"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics YOLOv8.1.10 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","Setup complete ✅ (2 CPUs, 12.7 GB RAM, 34.3/78.2 GB disk)\n"]}],"source":["\n","!pip install ultralytics\n","\n","from IPython import display\n","display.clear_output()\n","\n","import ultralytics\n","ultralytics.checks()\n","from ultralytics import YOLO\n","\n","from IPython.display import display, Image"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X61fM1yqWF34","outputId":"c0401771-d71b-49df-f5d9-104e7462a1bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8l-cls.pt to 'yolov8l-cls.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 71.7M/71.7M [00:00<00:00, 331MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Ultralytics YOLOv8.1.10 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8l-cls.pt, data=/content/dataset/IMG_CLASSES_BALANCED, epochs=50, time=None, patience=50, batch=16, imgsz=512, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/dataset/IMG_CLASSES_BALANCED/train... found 8400 images in 10 classes ✅ \n","\u001b[34m\u001b[1mval:\u001b[0m None...\n","\u001b[34m\u001b[1mtest:\u001b[0m /content/dataset/IMG_CLASSES_BALANCED/test... found 3600 images in 10 classes ✅ \n","Overriding model.yaml nc=1000 with nc=10\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n","  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n","  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n","  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n","  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n","  7                  -1  1   4720640  ultralytics.nn.modules.conv.Conv             [512, 1024, 3, 2]             \n","  8                  -1  3  17836032  ultralytics.nn.modules.block.C2f             [1024, 1024, 3, True]         \n","  9                  -1  1   1326090  ultralytics.nn.modules.head.Classify         [1024, 10]                    \n","YOLOv8l-cls summary: 183 layers, 36212554 parameters, 36212554 gradients, 99.1 GFLOPs\n","Transferred 300/302 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6.23M/6.23M [00:00<00:00, 160MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/dataset/IMG_CLASSES_BALANCED/train... 8400 images, 0 corrupt: 100%|██████████| 8400/8400 [00:02<00:00, 3080.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/dataset/IMG_CLASSES_BALANCED/train.cache\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/IMG_CLASSES_BALANCED/test... 3600 images, 0 corrupt: 100%|██████████| 3600/3600 [00:02<00:00, 1614.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/dataset/IMG_CLASSES_BALANCED/test.cache\n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 50 weight(decay=0.0), 51 weight(decay=0.0005), 51 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n","Image sizes 512 train, 512 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/classify/train\u001b[0m\n","Starting training for 50 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       1/50      4.84G      2.315         16        512:   1%|          | 5/525 [00:03<05:09,  1.68it/s]"]},{"output_type":"stream","name":"stdout","text":["Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"]},{"output_type":"stream","name":"stderr","text":["       1/50      5.04G      2.312         16        512:   1%|▏         | 7/525 [00:05<05:31,  1.56it/s]\n","100%|██████████| 755k/755k [00:00<00:00, 41.5MB/s]\n","       1/50      5.04G      1.597         16        512: 100%|██████████| 525/525 [04:23<00:00,  1.99it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 113/113 [00:42<00:00,  2.64it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.543      0.967\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       2/50      5.14G      1.175         16        512: 100%|██████████| 525/525 [04:15<00:00,  2.06it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 113/113 [00:42<00:00,  2.68it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.539      0.971\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       3/50      5.13G      1.132         16        512: 100%|██████████| 525/525 [04:13<00:00,  2.07it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 113/113 [00:42<00:00,  2.64it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.564      0.979\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       4/50      5.13G      1.018         16        512:   9%|▉         | 47/525 [00:23<03:11,  2.49it/s]"]}],"source":["import os\n","\n","from ultralytics import YOLO\n","\n","DATA_DIR = '/content/dataset/IMG_CLASSES_BALANCED'\n","# Load a model\n","model = YOLO(\"yolov8l-cls.pt\")  # load a pretained model\n","\n","# Use the model\n","results = model.train(data=DATA_DIR, epochs=50, imgsz=512)  # train the model\n"]},{"cell_type":"code","source":["import cv2\n","\n","# Load the image\n","image_path = '/content/0_29.jpg'\n","image = cv2.imread(image_path)\n","\n","# Check if the image was successfully loaded\n","if image is not None:\n","    # Define the desired dimensions\n","    desired_width = 320\n","    desired_height = 320\n","\n","    # Resize the image\n","    resized_image = cv2.resize(image, (desired_width, desired_height))\n","\n","    # Save or use the resized image\n","    cv2.imwrite('/content/exi.jpg', resized_image)\n","else:\n","    print(\"Image could not be loaded.\")\n"],"metadata":{"id":"v2iiYpeQDhwy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ultralytics import YOLO\n","\n","# Load a model\n","model = YOLO('yolov8n-cls.pt')  # load an official model\n","model = YOLO('/content/drive/MyDrive/YOLOALGO3Augmen/runs/classify/train/weights/best.pt')  # load a custom model\n","\n","# Predict with the model\n","results = model('/content/Getty.jpg')  # predict on an image"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lk8J0D3L-9Cs","executionInfo":{"status":"ok","timestamp":1704066860023,"user_tz":-120,"elapsed":3550,"user":{"displayName":"Team Project","userId":"02515086979699117879"}},"outputId":"502c3e3a-d52b-4c24-8381-d0a3310ddec6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","image 1/1 /content/Getty.jpg: 320x320 Warts Molluscum and other Viral Infections 0.77, Psoriasis pictures Lichen Planus and related diseases 0.09, Eczema 0.09, Tinea Ringworm Candidiasis and other Fungal Infections 0.04, Atopic Dermatitis 0.00, 517.6ms\n","Speed: 2.5ms preprocess, 517.6ms inference, 0.1ms postprocess per image at shape (1, 3, 320, 320)\n"]}]},{"cell_type":"code","source":["import json\n","\n","class_names = [\n","    \"Atopic Dermatitis\", \"Basal Cell Carcinoma (BCC)\", \"Benign Keratosis-like Lesions (BKL)\", \"Eczema\",\n","    \"Melanocytic Nevi (NV)\", \"Melanoma\", \"Psoriasis pictures Lichen Planus and related diseases\",\n","    \"Seborrheic Keratoses and other Benign Tumors\", \"Tinea Ringworm Candidiasis and other Fungal Infections\",\n","    \"Warts Molluscum and other Viral Infections\"\n","]\n","\n","metadata = {\n","    \"name\": \"Skin Disease Classification Model\",\n","    \"description\": \"A model for classifying skin diseases\",\n","    \"input\": [{\"name\": \"input_image\", \"type\": \"uint8\", \"shape\": [1, 3, 320, 320]}],\n","    \"output\": [{\"name\": \"output_scores\", \"type\": \"float32\", \"shape\": [1, len(class_names)]}],\n","    \"labels\": class_names\n","}\n","\n","with open('model_metadata.json', 'w') as meta_file:\n","    json.dump(metadata, meta_file)\n"],"metadata":{"id":"tJ8gXmAFHAxy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","def preprocess_image(image_path, target_shape=(320, 320)):\n","    # Load the image\n","    image = cv2.imread(image_path)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n","\n","    # Resize the image to the target shape\n","    resized_image = cv2.resize(image, (target_shape[1], target_shape[0]))\n","\n","    # Normalize pixel values to [0, 1] and reshape to match the required format (1, 3, 320, 320)\n","    input_image = resized_image.astype(np.float32) / 255.0\n","    input_image = np.transpose(input_image, (2, 0, 1))  # Change shape from (height, width, channels) to (channels, height, width)\n","    input_image = np.expand_dims(input_image, axis=0)   # Add a batch dimension\n","\n","    return input_image\n","\n","# Usage example:\n","image_path = '/content/drive/MyDrive/18_84.jpg'  # Replace with your image path\n","input_image = preprocess_image(image_path)\n","print(f\"Processed image shape: {input_image.shape}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E4-C7UwwIdhH","executionInfo":{"status":"ok","timestamp":1704138298941,"user_tz":-120,"elapsed":608,"user":{"displayName":"Team Project","userId":"02515086979699117879"}},"outputId":"8bb99268-e164-4c7d-a8ad-7c38e1ba276f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed image shape: (1, 3, 320, 320)\n"]}]},{"cell_type":"code","source":["from ultralytics import YOLO\n","\n","# Load a model\n","model = YOLO('yolov8n-cls.pt')  # load an official model\n","model = YOLO('/content/drive/MyDrive/YOLOALGO3Augmen/runs/classify/train/weights/best.pt')  # load a custom model\n","\n","# Export the model\n","model.export(format='onnx')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":489},"id":"snPKQRuqTGgd","executionInfo":{"status":"ok","timestamp":1704133993275,"user_tz":-120,"elapsed":20298,"user":{"displayName":"Team Project","userId":"02515086979699117879"}},"outputId":"52e53521-fb9d-4d5a-ad8c-7d138118a739"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics YOLOv8.0.232 🚀 Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.00GHz)\n","YOLOv8l-cls summary (fused): 133 layers, 36197386 parameters, 0 gradients, 98.7 GFLOPs\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/drive/MyDrive/YOLOALGO3Augmen/runs/classify/train/weights/best.pt' with input shape (1, 3, 320, 320) BCHW and output shape(s) (1, 10) (69.2 MB)\n","\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnx>=1.12.0'] not found, attempting AutoUpdate...\n","Collecting onnx>=1.12.0\n","  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/15.7 MB 264.0 MB/s eta 0:00:00\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (1.23.5)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (3.20.3)\n","Installing collected packages: onnx\n","Successfully installed onnx-1.15.0\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 9.7s, installed 1 package: ['onnx>=1.12.0']\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","\n","\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 17...\n","\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 16.7s, saved as '/content/drive/MyDrive/YOLOALGO3Augmen/runs/classify/train/weights/best.onnx' (138.1 MB)\n","\n","Export complete (19.5s)\n","Results saved to \u001b[1m/content/drive/MyDrive/YOLOALGO3Augmen/runs/classify/train/weights\u001b[0m\n","Predict:         yolo predict task=classify model=/content/drive/MyDrive/YOLOALGO3Augmen/runs/classify/train/weights/best.onnx imgsz=320  \n","Validate:        yolo val task=classify model=/content/drive/MyDrive/YOLOALGO3Augmen/runs/classify/train/weights/best.onnx imgsz=320 data=/content/dataset/IMG_CLASSES  \n","Visualize:       https://netron.app\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/YOLOALGO3Augmen/runs/classify/train/weights/best.onnx'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["import tensorflow as tf\n","from PIL import Image\n","import numpy as np\n","\n","# Load the TensorFlow Lite model\n","model_path = '/content/drive/MyDrive/YOLOTFLITE/converted_model.tflite'  # Replace with the path to your TensorFlow Lite model\n","interpreter = tf.lite.Interpreter(model_path=model_path)\n","interpreter.allocate_tensors()\n","\n","# Get input and output details (assuming single input and output)\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","\n","# Load and preprocess the image\n","image_path = '/content/mil.jpg'  # Replace with your image path\n","target_width, target_height = input_details[0]['shape'][2], input_details[0]['shape'][1]\n","\n","# Load the image and resize to match the model's input shape\n","image = Image.open(image_path)\n","image = image.resize((target_width, target_height))\n","\n","# Convert image to numpy array, normalize pixel values to [0, 1]\n","image = np.array(image, dtype=np.float32) / 255.0\n","\n","# Ensure image has the correct shape and format\n","image = np.expand_dims(image, axis=0)  # Add batch dimension\n","\n","# Transpose the image if needed (depends on model's input format)\n","# image = np.transpose(image, (0, 3, 1, 2))  # Adjust based on model's input format\n","\n","# Run inference\n","interpreter.set_tensor(input_details[0]['index'], image)\n","interpreter.invoke()\n","\n","# Get the output\n","output = interpreter.get_tensor(output_details[0]['index'])\n","\n","# Process the output as needed\n","# Output contains predictions or scores based on your model\n","\n","# Print the output (for illustration)\n","print(output)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":366},"id":"r8rdlN_-Nx2r","executionInfo":{"status":"error","timestamp":1704133752399,"user_tz":-120,"elapsed":1086,"user":{"displayName":"Team Project","userId":"02515086979699117879"}},"outputId":"df82f607-30e4-4663-fb56-4884328999f9"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-efbb5b5b1457>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36mset_tensor\u001b[0;34m(self, tensor_index, value)\u001b[0m\n\u001b[1;32m    718\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minterpreter\u001b[0m \u001b[0mcould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mset\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     \"\"\"\n\u001b[0;32m--> 720\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mresize_tensor_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Dimension mismatch. Got 3 but expected 320 for dimension 3 of input 0."]}]},{"cell_type":"code","source":[],"metadata":{"id":"XF7vjKy7Si-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ultralytics import YOLO\n","import json\n","\n","# Define the path to the model and metadata file\n","model_path = '/content/drive/MyDrive/YOLOTF1/converted_model.tflite'\n","metadata_path = '/content/model_metadata.json'\n","\n","# Load the model\n","model = YOLO(model_path, task='classify')\n","\n","# Load metadata separately\n","with open(metadata_path, 'r') as metadata_file:\n","    metadata = json.load(metadata_file)\n","\n","# Now you can use the metadata along with the model\n","# For example, accessing class names from metadata\n","class_names = metadata['labels']\n","\n","# Make predictions with the model\n","results = model('/content/mil.jpg')  # Replace 'mil.jpg' with your image path\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"id":"ZzSVMgzYBrJr","executionInfo":{"status":"error","timestamp":1704132433789,"user_tz":-120,"elapsed":1008,"user":{"displayName":"Team Project","userId":"02515086979699117879"}},"outputId":"670d06a1-f3ab-47a2-e07a-1c2939df1f56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading /content/drive/MyDrive/YOLOTF1/converted_model.tflite for TensorFlow Lite inference...\n","WARNING ⚠️ Metadata not found for 'model=/content/drive/MyDrive/YOLOTF1/converted_model.tflite'\n","\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-d3ed8f93b602>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Make predictions with the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/mil.jpg'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace 'mil.jpg' with your image path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;34m\"\"\"Calls the predict() method with given arguments to perform object detection.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprompts\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set_prompts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for SAM-type models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_cli\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# merge list of Result into one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mgenerator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Issuing `None` to a generator fires it up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36mstream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprofilers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpreds\u001b[0m  \u001b[0;31m# yield embedding tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m         visualize = increment_path(self.save_dir / Path(self.batch[0][0]).stem,\n\u001b[1;32m    136\u001b[0m                                    mkdir=True) if self.args.visualize and (not self.source_type.tensor) else False\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/autobackend.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    433\u001b[0m                     \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quantization'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mzero_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dtype'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# de-scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36mset_tensor\u001b[0;34m(self, tensor_index, value)\u001b[0m\n\u001b[1;32m    718\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minterpreter\u001b[0m \u001b[0mcould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mset\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     \"\"\"\n\u001b[0;32m--> 720\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mresize_tensor_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Dimension mismatch. Got 640 but expected 3 for dimension 1 of input 0."]}]},{"cell_type":"code","source":["!pip install onnx-tf\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8FOsoK8jTwuK","executionInfo":{"status":"ok","timestamp":1704134131501,"user_tz":-120,"elapsed":8180,"user":{"displayName":"Team Project","userId":"02515086979699117879"}},"outputId":"b598420e-2a4b-4754-8400-c864afdd8115"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnx-tf\n","  Downloading onnx_tf-1.10.0-py3-none-any.whl (226 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.1/226.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: onnx>=1.10.2 in /usr/local/lib/python3.10/dist-packages (from onnx-tf) (1.15.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from onnx-tf) (6.0.1)\n","Collecting tensorflow-addons (from onnx-tf)\n","  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx>=1.10.2->onnx-tf) (1.23.5)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.10.2->onnx-tf) (3.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons->onnx-tf) (23.2)\n","Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->onnx-tf)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Installing collected packages: typeguard, tensorflow-addons, onnx-tf\n","Successfully installed onnx-tf-1.10.0 tensorflow-addons-0.23.0 typeguard-2.13.3\n"]}]},{"cell_type":"code","source":["import onnx\n","import tensorflow as tf\n","from onnx_tf.backend import prepare\n","\n","# Load ONNX model\n","onnx_model = onnx.load('/content/drive/MyDrive/YOLOALGO3Augmen/runs/classify/train/weights/best.onnx')\n","\n","# Prepare ONNX model for TensorFlow\n","tf_rep = prepare(onnx_model)\n","tf_rep.export_graph('/content/drive/MyDrive/YOLOALGO3Augmen/runs/classify/train/weights/yolov8.pb')\n","\n","# Convert TensorFlow model (.pb) to TensorFlow Lite with TF Select enabled\n","converter = tf.lite.TFLiteConverter.from_saved_model('/content/drive/MyDrive/YOLOALGO3Augmen/runs/classify/train/weights/yolov8.pb')\n","converter.experimental_new_converter = True  # Enable the new converter\n","converter.target_spec.supported_ops = [\n","    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite built-in ops\n","    tf.lite.OpsSet.SELECT_TF_OPS  # Enable TF Select for handling unsupported ops\n","]\n","tflite_model = converter.convert()\n","\n","# Save TensorFlow Lite model\n","tflite_model_file = '/content/drive/MyDrive/YOLOALGO3Augmen/runs/classify/train/weights/yolov8.tflite'\n","with open(tflite_model_file, 'wb') as f:\n","    f.write(tflite_model)\n"],"metadata":{"id":"9_pZWhbzTegG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8Q66yxYRdHm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from PIL import Image\n","\n","# Load the TensorFlow Lite model\n","tflite_model_file = '/content/drive/MyDrive/YOLOTF1/converted_model.tflite'\n","interpreter = tf.lite.Interpreter(model_path=tflite_model_file)\n","interpreter.allocate_tensors()\n","\n","# Get input and output details (assuming single input and output)\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","\n","# Load and preprocess the image\n","image_path = '/content/drive/MyDrive/mm.jpg'  # Replace with your image path\n","input_shape = (320, 320)  # Model's input shape\n","\n","# Load the image and resize to match the model's input shape\n","image = Image.open(image_path)\n","image = image.convert(\"RGB\")  # Ensure RGB format\n","image = image.resize(input_shape)\n","\n","# Normalize pixel values to [0, 1] and convert to numpy array\n","image = np.array(image, dtype=np.float32) / 255.0\n","\n","# Rearrange dimensions to match the model's expected input shape [1, 3, 320, 320]\n","input_data = np.moveaxis(image, -1, 0)  # Move channel axis to front\n","input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n","\n","# Check if the input shape matches the model's expected input shape\n","if tuple(input_details[0]['shape']) != input_data.shape:\n","    raise ValueError(\n","        f\"Input shape mismatch, expected {input_details[0]['shape']} but got {input_data.shape}\"\n","    )\n","\n","# Set the input tensor\n","interpreter.set_tensor(input_details[0]['index'], input_data)\n","\n","# Run inference\n","interpreter.invoke()\n","\n","# Get the output tensor\n","output_data = interpreter.get_tensor(output_details[0]['index'])\n","predicted_class_index = np.argmax(output_data)  # Get index of highest probability class\n","predicted_class_name = class_names[predicted_class_index]  # Get the class name\n","\n","# Display the predicted class name\n","print(f\"The predicted class is: {predicted_class_name}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y3wqDcmJWBqi","executionInfo":{"status":"ok","timestamp":1704138533697,"user_tz":-120,"elapsed":2650,"user":{"displayName":"Team Project","userId":"02515086979699117879"}},"outputId":"7b562d1e-e635-45ba-a0d9-382450393bd0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The predicted class is: Tinea Ringworm Candidiasis and other Fungal Infections\n"]}]},{"cell_type":"code","source":["import zipfile\n","\n","def compress_model(input_file_name, output_file_name):\n","    with zipfile.ZipFile(output_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","        zipf.write(input_file_name, arcname=input_file_name)\n","\n","# Specify the name of your existing .tflite file\n","input_file_name = '/content/drive/MyDrive/YOLOALGO3Augmen/runs/classify/train/weights/yolov8.tflite'\n","# Specify the desired output zip file name\n","output_file_name = '/content/drive/MyDrive/YOLOALGO3Augmen/runs/classify/train/weights/model_compressed.zip'\n","\n","compress_model(input_file_name, output_file_name)\n"],"metadata":{"id":"hvCpCJqbstr7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def decompress_model(input_zip_file, extract_folder='.'):\n","    with zipfile.ZipFile(input_zip_file, 'r') as zip_ref:\n","        zip_ref.extractall(extract_folder)\n","\n","# Specify the name of your compressed file and the extraction folder\n","input_zip_file = 'model_compressed.zip'\n","extract_folder = '.'  # Current directory\n","\n","decompress_model(input_zip_file, extract_folder)\n"],"metadata":{"id":"f-ZkzX6vupru"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNJBMfpNRN1Rw8oMG4dwKg/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}